{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-23T14:54:22.366046Z",
     "start_time": "2024-09-23T14:54:22.363120Z"
    }
   },
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from utils_io import read_bitarrays\n",
    "from utils_plot import plot_receiver_operating_characteristics_curve\n",
    "from utils_classification import PoolAttackerDataset, LSTMAttacker, LSTMAttackerTrainer"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "num_snps = 40000",
   "id": "b353e50c759e6e27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:54:22.614356Z",
     "start_time": "2024-09-23T14:54:22.373051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "genomes_pool = read_bitarrays(\"../data/test/In_Pop.pkl\")[:, :num_snps]\n",
    "genomes_reference = read_bitarrays(\"../data/test/Not_In_Pop.pkl\")[:, :num_snps]\n",
    "genomes = np.concatenate((genomes_pool, genomes_reference), axis=0)"
   ],
   "id": "6f6f40c2dcb7e558",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:54:23.355941Z",
     "start_time": "2024-09-23T14:54:23.067532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels_pool = np.ones(genomes_pool.shape[0], dtype=bool)\n",
    "labels_reference = np.zeros(genomes_reference.shape[0], dtype=bool)\n",
    "labels = np.concatenate((labels_pool, labels_reference), axis=0).astype(bool)"
   ],
   "id": "ae56d7fe01f7902e",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "frequencies_pool = np.mean(genomes_pool, axis=0)\n",
    "frequencies_reference = np.mean(genomes_reference, axis=0)"
   ],
   "id": "e1780e867b9b876d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:54:24.049496Z",
     "start_time": "2024-09-23T14:54:23.890604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = PoolAttackerDataset(\n",
    "    target_genomes=genomes,\n",
    "    pool_frequencies=frequencies_pool,\n",
    "    reference_frequencies=frequencies_reference,\n",
    "    labels=labels)"
   ],
   "id": "2ffb68fc3c54c08f",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:54:24.574247Z",
     "start_time": "2024-09-23T14:54:24.551082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_genomes = len(dataset)\n",
    "num_genomes_train = int(0.7 * num_genomes)\n",
    "num_genomes_eval = int(0.15 * num_genomes)\n",
    "num_genomes_test = num_genomes - num_genomes_train - num_genomes_eval"
   ],
   "id": "ed0251c984651915",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset_train, dataset_eval, dataset_test = random_split(\n",
    "    dataset,\n",
    "    [num_genomes_train, num_genomes_eval, num_genomes_test])"
   ],
   "id": "ec2841d980d432d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:54:24.644380Z",
     "start_time": "2024-09-23T14:54:24.641417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "6088299d7bb0865e",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:54:24.722713Z",
     "start_time": "2024-09-23T14:54:24.716946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LSTMAttacker(input_size=2, hidden_size=64, num_layers=1, bidirectional=False, dropout=0.5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ],
   "id": "64b2ad3e77b74873",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attacker(\n",
       "  (lstm): LSTM(2, 64, batch_first=True)\n",
       "  (linear): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T14:54:24.793163Z",
     "start_time": "2024-09-23T14:54:24.790373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "49c6480752277241",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T15:02:01.786918Z",
     "start_time": "2024-09-23T14:54:24.853955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 256\n",
    "best_epoch = -1\n",
    "best_val_loss = np.inf\n",
    "best_state_dict = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            val_losses.append(loss.item())\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_epoch = epoch\n",
    "        best_val_loss = val_loss\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "model.load_state_dict(best_state_dict)\n",
    "print(f\"Best Model found at Epoch {best_epoch + 1}\")"
   ],
   "id": "2fd608b5cd596024",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256, Train Loss: 0.6939, Val Loss: 0.6932\n",
      "Epoch 2/256, Train Loss: 0.6933, Val Loss: 0.6932\n",
      "Epoch 3/256, Train Loss: 0.6933, Val Loss: 0.6931\n",
      "Epoch 4/256, Train Loss: 0.6934, Val Loss: 0.6932\n",
      "Epoch 5/256, Train Loss: 0.6939, Val Loss: 0.6931\n",
      "Epoch 6/256, Train Loss: 0.6932, Val Loss: 0.6931\n",
      "Epoch 7/256, Train Loss: 0.6937, Val Loss: 0.6931\n",
      "Epoch 8/256, Train Loss: 0.6935, Val Loss: 0.6931\n",
      "Epoch 9/256, Train Loss: 0.6938, Val Loss: 0.6931\n",
      "Epoch 10/256, Train Loss: 0.6933, Val Loss: 0.6930\n",
      "Epoch 11/256, Train Loss: 0.6936, Val Loss: 0.6931\n",
      "Epoch 12/256, Train Loss: 0.6932, Val Loss: 0.6930\n",
      "Epoch 13/256, Train Loss: 0.6932, Val Loss: 0.6930\n",
      "Epoch 14/256, Train Loss: 0.6932, Val Loss: 0.6929\n",
      "Epoch 15/256, Train Loss: 0.6932, Val Loss: 0.6929\n",
      "Epoch 16/256, Train Loss: 0.6933, Val Loss: 0.6929\n",
      "Epoch 17/256, Train Loss: 0.6933, Val Loss: 0.6929\n",
      "Epoch 18/256, Train Loss: 0.6932, Val Loss: 0.6928\n",
      "Epoch 19/256, Train Loss: 0.6934, Val Loss: 0.6928\n",
      "Epoch 20/256, Train Loss: 0.6931, Val Loss: 0.6928\n",
      "Epoch 21/256, Train Loss: 0.6933, Val Loss: 0.6927\n",
      "Epoch 22/256, Train Loss: 0.6931, Val Loss: 0.6926\n",
      "Epoch 23/256, Train Loss: 0.6931, Val Loss: 0.6925\n",
      "Epoch 24/256, Train Loss: 0.6937, Val Loss: 0.6925\n",
      "Epoch 25/256, Train Loss: 0.6931, Val Loss: 0.6923\n",
      "Epoch 26/256, Train Loss: 0.6930, Val Loss: 0.6922\n",
      "Epoch 27/256, Train Loss: 0.6928, Val Loss: 0.6919\n",
      "Epoch 28/256, Train Loss: 0.6928, Val Loss: 0.6913\n",
      "Epoch 29/256, Train Loss: 0.6927, Val Loss: 0.6910\n",
      "Epoch 30/256, Train Loss: 0.6930, Val Loss: 0.6912\n",
      "Epoch 31/256, Train Loss: 0.6925, Val Loss: 0.6909\n",
      "Epoch 32/256, Train Loss: 0.6925, Val Loss: 0.6902\n",
      "Epoch 33/256, Train Loss: 0.6925, Val Loss: 0.6901\n",
      "Epoch 34/256, Train Loss: 0.6928, Val Loss: 0.6892\n",
      "Epoch 35/256, Train Loss: 0.6919, Val Loss: 0.6897\n",
      "Epoch 36/256, Train Loss: 0.6927, Val Loss: 0.6894\n",
      "Epoch 37/256, Train Loss: 0.6920, Val Loss: 0.6892\n",
      "Epoch 38/256, Train Loss: 0.6919, Val Loss: 0.6883\n",
      "Epoch 39/256, Train Loss: 0.6926, Val Loss: 0.6873\n",
      "Epoch 40/256, Train Loss: 0.6915, Val Loss: 0.6883\n",
      "Epoch 41/256, Train Loss: 0.6917, Val Loss: 0.6874\n",
      "Epoch 42/256, Train Loss: 0.6912, Val Loss: 0.6874\n",
      "Epoch 43/256, Train Loss: 0.6912, Val Loss: 0.6865\n",
      "Epoch 44/256, Train Loss: 0.6911, Val Loss: 0.6881\n",
      "Epoch 45/256, Train Loss: 0.6921, Val Loss: 0.6886\n",
      "Epoch 46/256, Train Loss: 0.6917, Val Loss: 0.6881\n",
      "Epoch 47/256, Train Loss: 0.6916, Val Loss: 0.6877\n",
      "Epoch 48/256, Train Loss: 0.6908, Val Loss: 0.6875\n",
      "Epoch 49/256, Train Loss: 0.6917, Val Loss: 0.6868\n",
      "Epoch 50/256, Train Loss: 0.6905, Val Loss: 0.6851\n",
      "Epoch 51/256, Train Loss: 0.6890, Val Loss: 0.6862\n",
      "Epoch 52/256, Train Loss: 0.6917, Val Loss: 0.6847\n",
      "Epoch 53/256, Train Loss: 0.6898, Val Loss: 0.6872\n",
      "Epoch 54/256, Train Loss: 0.6898, Val Loss: 0.6863\n",
      "Epoch 55/256, Train Loss: 0.6886, Val Loss: 0.6857\n",
      "Epoch 56/256, Train Loss: 0.6887, Val Loss: 0.6870\n",
      "Epoch 57/256, Train Loss: 0.6895, Val Loss: 0.6862\n",
      "Epoch 58/256, Train Loss: 0.6935, Val Loss: 0.6870\n",
      "Epoch 59/256, Train Loss: 0.6910, Val Loss: 0.6928\n",
      "Epoch 60/256, Train Loss: 0.6936, Val Loss: 0.6919\n",
      "Epoch 61/256, Train Loss: 0.6931, Val Loss: 0.6913\n",
      "Epoch 62/256, Train Loss: 0.6929, Val Loss: 0.6910\n",
      "Epoch 63/256, Train Loss: 0.6930, Val Loss: 0.6911\n",
      "Epoch 64/256, Train Loss: 0.6926, Val Loss: 0.6906\n",
      "Epoch 65/256, Train Loss: 0.6932, Val Loss: 0.6904\n",
      "Epoch 66/256, Train Loss: 0.6925, Val Loss: 0.6901\n",
      "Epoch 67/256, Train Loss: 0.6928, Val Loss: 0.6899\n",
      "Epoch 68/256, Train Loss: 0.6924, Val Loss: 0.6897\n",
      "Epoch 69/256, Train Loss: 0.6925, Val Loss: 0.6897\n",
      "Epoch 70/256, Train Loss: 0.6925, Val Loss: 0.6897\n",
      "Epoch 71/256, Train Loss: 0.6925, Val Loss: 0.6893\n",
      "Epoch 72/256, Train Loss: 0.6922, Val Loss: 0.6892\n",
      "Epoch 73/256, Train Loss: 0.6927, Val Loss: 0.6893\n",
      "Epoch 74/256, Train Loss: 0.6921, Val Loss: 0.6890\n",
      "Epoch 75/256, Train Loss: 0.6923, Val Loss: 0.6889\n",
      "Epoch 76/256, Train Loss: 0.6923, Val Loss: 0.6889\n",
      "Epoch 77/256, Train Loss: 0.6920, Val Loss: 0.6888\n",
      "Epoch 78/256, Train Loss: 0.6920, Val Loss: 0.6886\n",
      "Epoch 79/256, Train Loss: 0.6919, Val Loss: 0.6884\n",
      "Epoch 80/256, Train Loss: 0.6920, Val Loss: 0.6885\n",
      "Epoch 81/256, Train Loss: 0.6920, Val Loss: 0.6883\n",
      "Epoch 82/256, Train Loss: 0.6919, Val Loss: 0.6882\n",
      "Epoch 83/256, Train Loss: 0.6919, Val Loss: 0.6883\n",
      "Epoch 84/256, Train Loss: 0.6919, Val Loss: 0.6883\n",
      "Epoch 85/256, Train Loss: 0.6919, Val Loss: 0.6880\n",
      "Epoch 86/256, Train Loss: 0.6919, Val Loss: 0.6880\n",
      "Epoch 87/256, Train Loss: 0.6917, Val Loss: 0.6879\n",
      "Epoch 88/256, Train Loss: 0.6918, Val Loss: 0.6879\n",
      "Epoch 89/256, Train Loss: 0.6918, Val Loss: 0.6878\n",
      "Epoch 90/256, Train Loss: 0.6918, Val Loss: 0.6876\n",
      "Epoch 91/256, Train Loss: 0.6919, Val Loss: 0.6878\n",
      "Epoch 92/256, Train Loss: 0.6921, Val Loss: 0.6875\n",
      "Epoch 93/256, Train Loss: 0.6918, Val Loss: 0.6874\n",
      "Epoch 94/256, Train Loss: 0.6917, Val Loss: 0.6874\n",
      "Epoch 95/256, Train Loss: 0.6916, Val Loss: 0.6874\n",
      "Epoch 96/256, Train Loss: 0.6918, Val Loss: 0.6874\n",
      "Epoch 97/256, Train Loss: 0.6916, Val Loss: 0.6872\n",
      "Epoch 98/256, Train Loss: 0.6916, Val Loss: 0.6873\n",
      "Epoch 99/256, Train Loss: 0.6916, Val Loss: 0.6871\n",
      "Epoch 100/256, Train Loss: 0.6917, Val Loss: 0.6871\n",
      "Epoch 101/256, Train Loss: 0.6916, Val Loss: 0.6871\n",
      "Epoch 102/256, Train Loss: 0.6919, Val Loss: 0.6870\n",
      "Epoch 103/256, Train Loss: 0.6915, Val Loss: 0.6870\n",
      "Epoch 104/256, Train Loss: 0.6917, Val Loss: 0.6870\n",
      "Epoch 105/256, Train Loss: 0.6915, Val Loss: 0.6869\n",
      "Epoch 106/256, Train Loss: 0.6915, Val Loss: 0.6870\n",
      "Epoch 107/256, Train Loss: 0.6915, Val Loss: 0.6870\n",
      "Epoch 108/256, Train Loss: 0.6917, Val Loss: 0.6869\n",
      "Epoch 109/256, Train Loss: 0.6915, Val Loss: 0.6869\n",
      "Epoch 110/256, Train Loss: 0.6915, Val Loss: 0.6868\n",
      "Epoch 111/256, Train Loss: 0.6915, Val Loss: 0.6869\n",
      "Epoch 112/256, Train Loss: 0.6915, Val Loss: 0.6868\n",
      "Epoch 113/256, Train Loss: 0.6914, Val Loss: 0.6868\n",
      "Epoch 114/256, Train Loss: 0.6915, Val Loss: 0.6868\n",
      "Epoch 115/256, Train Loss: 0.6915, Val Loss: 0.6867\n",
      "Epoch 116/256, Train Loss: 0.6914, Val Loss: 0.6867\n",
      "Epoch 117/256, Train Loss: 0.6914, Val Loss: 0.6868\n",
      "Epoch 118/256, Train Loss: 0.6915, Val Loss: 0.6867\n",
      "Epoch 119/256, Train Loss: 0.6914, Val Loss: 0.6867\n",
      "Epoch 120/256, Train Loss: 0.6914, Val Loss: 0.6866\n",
      "Epoch 121/256, Train Loss: 0.6913, Val Loss: 0.6866\n",
      "Epoch 122/256, Train Loss: 0.6915, Val Loss: 0.6866\n",
      "Epoch 123/256, Train Loss: 0.6913, Val Loss: 0.6866\n",
      "Epoch 124/256, Train Loss: 0.6914, Val Loss: 0.6865\n",
      "Epoch 125/256, Train Loss: 0.6915, Val Loss: 0.6866\n",
      "Epoch 126/256, Train Loss: 0.6913, Val Loss: 0.6865\n",
      "Epoch 127/256, Train Loss: 0.6913, Val Loss: 0.6866\n",
      "Epoch 128/256, Train Loss: 0.6916, Val Loss: 0.6865\n",
      "Epoch 129/256, Train Loss: 0.6913, Val Loss: 0.6865\n",
      "Epoch 130/256, Train Loss: 0.6913, Val Loss: 0.6866\n",
      "Epoch 131/256, Train Loss: 0.6913, Val Loss: 0.6865\n",
      "Epoch 132/256, Train Loss: 0.6913, Val Loss: 0.6864\n",
      "Epoch 133/256, Train Loss: 0.6913, Val Loss: 0.6865\n",
      "Epoch 134/256, Train Loss: 0.6913, Val Loss: 0.6865\n",
      "Epoch 135/256, Train Loss: 0.6914, Val Loss: 0.6865\n",
      "Epoch 136/256, Train Loss: 0.6916, Val Loss: 0.6864\n",
      "Epoch 137/256, Train Loss: 0.6914, Val Loss: 0.6864\n",
      "Epoch 138/256, Train Loss: 0.6913, Val Loss: 0.6864\n",
      "Epoch 139/256, Train Loss: 0.6913, Val Loss: 0.6864\n",
      "Epoch 140/256, Train Loss: 0.6912, Val Loss: 0.6864\n",
      "Epoch 141/256, Train Loss: 0.6912, Val Loss: 0.6864\n",
      "Epoch 142/256, Train Loss: 0.6913, Val Loss: 0.6864\n",
      "Epoch 143/256, Train Loss: 0.6912, Val Loss: 0.6864\n",
      "Epoch 144/256, Train Loss: 0.6913, Val Loss: 0.6864\n",
      "Epoch 145/256, Train Loss: 0.6911, Val Loss: 0.6863\n",
      "Epoch 146/256, Train Loss: 0.6912, Val Loss: 0.6864\n",
      "Epoch 147/256, Train Loss: 0.6912, Val Loss: 0.6863\n",
      "Epoch 148/256, Train Loss: 0.6912, Val Loss: 0.6864\n",
      "Epoch 149/256, Train Loss: 0.6911, Val Loss: 0.6864\n",
      "Epoch 150/256, Train Loss: 0.6911, Val Loss: 0.6863\n",
      "Epoch 151/256, Train Loss: 0.6911, Val Loss: 0.6864\n",
      "Epoch 152/256, Train Loss: 0.6913, Val Loss: 0.6863\n",
      "Epoch 153/256, Train Loss: 0.6911, Val Loss: 0.6863\n",
      "Epoch 154/256, Train Loss: 0.6912, Val Loss: 0.6863\n",
      "Epoch 155/256, Train Loss: 0.6911, Val Loss: 0.6863\n",
      "Epoch 156/256, Train Loss: 0.6912, Val Loss: 0.6864\n",
      "Epoch 157/256, Train Loss: 0.6911, Val Loss: 0.6863\n",
      "Epoch 158/256, Train Loss: 0.6911, Val Loss: 0.6863\n",
      "Epoch 159/256, Train Loss: 0.6911, Val Loss: 0.6863\n",
      "Epoch 160/256, Train Loss: 0.6911, Val Loss: 0.6863\n",
      "Epoch 161/256, Train Loss: 0.6910, Val Loss: 0.6863\n",
      "Epoch 162/256, Train Loss: 0.6911, Val Loss: 0.6864\n",
      "Epoch 163/256, Train Loss: 0.6910, Val Loss: 0.6863\n",
      "Epoch 164/256, Train Loss: 0.6911, Val Loss: 0.6862\n",
      "Epoch 165/256, Train Loss: 0.6911, Val Loss: 0.6862\n",
      "Epoch 166/256, Train Loss: 0.6910, Val Loss: 0.6863\n",
      "Epoch 167/256, Train Loss: 0.6910, Val Loss: 0.6862\n",
      "Epoch 168/256, Train Loss: 0.6910, Val Loss: 0.6862\n",
      "Epoch 169/256, Train Loss: 0.6909, Val Loss: 0.6862\n",
      "Epoch 170/256, Train Loss: 0.6910, Val Loss: 0.6863\n",
      "Epoch 171/256, Train Loss: 0.6909, Val Loss: 0.6862\n",
      "Epoch 172/256, Train Loss: 0.6910, Val Loss: 0.6863\n",
      "Epoch 173/256, Train Loss: 0.6909, Val Loss: 0.6862\n",
      "Epoch 174/256, Train Loss: 0.6910, Val Loss: 0.6862\n",
      "Epoch 175/256, Train Loss: 0.6910, Val Loss: 0.6862\n",
      "Epoch 176/256, Train Loss: 0.6913, Val Loss: 0.6863\n",
      "Epoch 177/256, Train Loss: 0.6908, Val Loss: 0.6862\n",
      "Epoch 178/256, Train Loss: 0.6908, Val Loss: 0.6862\n",
      "Epoch 179/256, Train Loss: 0.6908, Val Loss: 0.6861\n",
      "Epoch 180/256, Train Loss: 0.6908, Val Loss: 0.6861\n",
      "Epoch 181/256, Train Loss: 0.6910, Val Loss: 0.6861\n",
      "Epoch 182/256, Train Loss: 0.6909, Val Loss: 0.6861\n",
      "Epoch 183/256, Train Loss: 0.6908, Val Loss: 0.6862\n",
      "Epoch 184/256, Train Loss: 0.6907, Val Loss: 0.6861\n",
      "Epoch 185/256, Train Loss: 0.6907, Val Loss: 0.6861\n",
      "Epoch 186/256, Train Loss: 0.6908, Val Loss: 0.6861\n",
      "Epoch 187/256, Train Loss: 0.6908, Val Loss: 0.6862\n",
      "Epoch 188/256, Train Loss: 0.6907, Val Loss: 0.6861\n",
      "Epoch 189/256, Train Loss: 0.6907, Val Loss: 0.6861\n",
      "Epoch 190/256, Train Loss: 0.6907, Val Loss: 0.6861\n",
      "Epoch 191/256, Train Loss: 0.6908, Val Loss: 0.6860\n",
      "Epoch 192/256, Train Loss: 0.6907, Val Loss: 0.6861\n",
      "Epoch 193/256, Train Loss: 0.6906, Val Loss: 0.6861\n",
      "Epoch 194/256, Train Loss: 0.6906, Val Loss: 0.6860\n",
      "Epoch 195/256, Train Loss: 0.6907, Val Loss: 0.6861\n",
      "Epoch 196/256, Train Loss: 0.6907, Val Loss: 0.6860\n",
      "Epoch 197/256, Train Loss: 0.6906, Val Loss: 0.6860\n",
      "Epoch 198/256, Train Loss: 0.6905, Val Loss: 0.6860\n",
      "Epoch 199/256, Train Loss: 0.6905, Val Loss: 0.6859\n",
      "Epoch 200/256, Train Loss: 0.6905, Val Loss: 0.6860\n",
      "Epoch 201/256, Train Loss: 0.6904, Val Loss: 0.6860\n",
      "Epoch 202/256, Train Loss: 0.6905, Val Loss: 0.6859\n",
      "Epoch 203/256, Train Loss: 0.6905, Val Loss: 0.6859\n",
      "Epoch 204/256, Train Loss: 0.6904, Val Loss: 0.6859\n",
      "Epoch 205/256, Train Loss: 0.6904, Val Loss: 0.6859\n",
      "Epoch 206/256, Train Loss: 0.6904, Val Loss: 0.6858\n",
      "Epoch 207/256, Train Loss: 0.6903, Val Loss: 0.6858\n",
      "Epoch 208/256, Train Loss: 0.6903, Val Loss: 0.6857\n",
      "Epoch 209/256, Train Loss: 0.6903, Val Loss: 0.6859\n",
      "Epoch 210/256, Train Loss: 0.6902, Val Loss: 0.6858\n",
      "Epoch 211/256, Train Loss: 0.6902, Val Loss: 0.6858\n",
      "Epoch 212/256, Train Loss: 0.6901, Val Loss: 0.6858\n",
      "Epoch 213/256, Train Loss: 0.6901, Val Loss: 0.6858\n",
      "Epoch 214/256, Train Loss: 0.6901, Val Loss: 0.6857\n",
      "Epoch 215/256, Train Loss: 0.6901, Val Loss: 0.6857\n",
      "Epoch 216/256, Train Loss: 0.6900, Val Loss: 0.6856\n",
      "Epoch 217/256, Train Loss: 0.6900, Val Loss: 0.6856\n",
      "Epoch 218/256, Train Loss: 0.6899, Val Loss: 0.6856\n",
      "Epoch 219/256, Train Loss: 0.6899, Val Loss: 0.6855\n",
      "Epoch 220/256, Train Loss: 0.6900, Val Loss: 0.6855\n",
      "Epoch 221/256, Train Loss: 0.6898, Val Loss: 0.6855\n",
      "Epoch 222/256, Train Loss: 0.6898, Val Loss: 0.6855\n",
      "Epoch 223/256, Train Loss: 0.6898, Val Loss: 0.6855\n",
      "Epoch 224/256, Train Loss: 0.6896, Val Loss: 0.6854\n",
      "Epoch 225/256, Train Loss: 0.6897, Val Loss: 0.6854\n",
      "Epoch 226/256, Train Loss: 0.6897, Val Loss: 0.6854\n",
      "Epoch 227/256, Train Loss: 0.6896, Val Loss: 0.6854\n",
      "Epoch 228/256, Train Loss: 0.6895, Val Loss: 0.6852\n",
      "Epoch 229/256, Train Loss: 0.6895, Val Loss: 0.6853\n",
      "Epoch 230/256, Train Loss: 0.6896, Val Loss: 0.6853\n",
      "Epoch 231/256, Train Loss: 0.6894, Val Loss: 0.6852\n",
      "Epoch 232/256, Train Loss: 0.6894, Val Loss: 0.6852\n",
      "Epoch 233/256, Train Loss: 0.6894, Val Loss: 0.6851\n",
      "Epoch 234/256, Train Loss: 0.6893, Val Loss: 0.6851\n",
      "Epoch 235/256, Train Loss: 0.6892, Val Loss: 0.6852\n",
      "Epoch 236/256, Train Loss: 0.6891, Val Loss: 0.6850\n",
      "Epoch 237/256, Train Loss: 0.6890, Val Loss: 0.6849\n",
      "Epoch 238/256, Train Loss: 0.6893, Val Loss: 0.6849\n",
      "Epoch 239/256, Train Loss: 0.6890, Val Loss: 0.6847\n",
      "Epoch 240/256, Train Loss: 0.6891, Val Loss: 0.6850\n",
      "Epoch 241/256, Train Loss: 0.6887, Val Loss: 0.6848\n",
      "Epoch 242/256, Train Loss: 0.6888, Val Loss: 0.6848\n",
      "Epoch 243/256, Train Loss: 0.6889, Val Loss: 0.6847\n",
      "Epoch 244/256, Train Loss: 0.6888, Val Loss: 0.6847\n",
      "Epoch 245/256, Train Loss: 0.6888, Val Loss: 0.6846\n",
      "Epoch 246/256, Train Loss: 0.6886, Val Loss: 0.6845\n",
      "Epoch 247/256, Train Loss: 0.6888, Val Loss: 0.6841\n",
      "Epoch 248/256, Train Loss: 0.6882, Val Loss: 0.6842\n",
      "Epoch 249/256, Train Loss: 0.6885, Val Loss: 0.6843\n",
      "Epoch 250/256, Train Loss: 0.6882, Val Loss: 0.6842\n",
      "Epoch 251/256, Train Loss: 0.6883, Val Loss: 0.6837\n",
      "Epoch 252/256, Train Loss: 0.6885, Val Loss: 0.6833\n",
      "Epoch 253/256, Train Loss: 0.6878, Val Loss: 0.6837\n",
      "Epoch 254/256, Train Loss: 0.6877, Val Loss: 0.6836\n",
      "Epoch 255/256, Train Loss: 0.6876, Val Loss: 0.6833\n",
      "Epoch 256/256, Train Loss: 0.6881, Val Loss: 0.6837\n",
      "Best Model found at Epoch 252\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T15:02:01.864010Z",
     "start_time": "2024-09-23T15:02:01.843799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save(\"../models\", f\"pool_lstm_attacker_{datetime.now().strftime('%Y%m%d%H%M%S')}\")\n",
    "# model.load(\"../models\", \"attacker_20240916085039\")"
   ],
   "id": "4be6219e67c78e12",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T15:02:02.118557Z",
     "start_time": "2024-09-23T15:02:01.917854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "test_losses = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        output = model(x_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        test_losses.append(loss.item())\n",
    "        predictions = (torch.sigmoid(output) >= 0.5).float()\n",
    "        correct += (predictions == y_batch).sum().item()\n",
    "        total += len(y_batch)\n",
    "test_loss = np.mean(test_losses)\n",
    "test_accuracy = correct / total"
   ],
   "id": "bf16a544fc227d5f",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T15:02:02.174925Z",
     "start_time": "2024-09-23T15:02:02.171928Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")",
   "id": "3990689652207d39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6833, Test Accuracy: 0.5813\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T15:02:02.250050Z",
     "start_time": "2024-09-23T15:02:02.246823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chance = y_test.mean()\n",
    "print(f\"Chance: {chance}\")"
   ],
   "id": "202b0182c6ea8b8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chance: 0.5\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T15:02:02.305409Z",
     "start_time": "2024-09-23T15:02:02.303902Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5bb481157c333bf8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
